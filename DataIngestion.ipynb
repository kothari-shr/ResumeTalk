{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ba66b804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Web Based Loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "loader = WebBaseLoader(web_paths=(\"https://docs.github.com/en/github-models/use-github-models/prototyping-with-ai-models\",),\n",
    "                       bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                           id=(\"article-contents\")\n",
    "                       ))\n",
    "                       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "738d7510",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.github.com/en/github-models/use-github-models/prototyping-with-ai-models'}, page_content='If you want to develop a generative AI application, you can use GitHub Models to find and experiment with AI models for free. Once you are ready to bring your application to production, opt in to paid usage for your enterprise.\\nOrganization owners can integrate their preferred custom models into GitHub Models, by using an organization\\'s own LLM API keys. See Using your own API keys in GitHub Models.\\nSee also Responsible use of GitHub Models.\\nFinding AI models\\nTo find an AI model:\\n\\n\\nGo to github.com/marketplace/models.\\n\\n\\nClick Model: Select a Model at the top left of the page.\\n\\n\\nChoose a model from the dropdown menu.\\nAlternatively, in the dropdown menu, click View all models, click a model in the Marketplace, then click  Playground.\\n\\n\\nThe model is opened in the model playground. Details of the model are displayed in the sidebar on the right. If the sidebar is not displayed, expand it by clicking the  icon at the right of the playground.\\nNote\\n Access to OpenAI\\'s models is in public preview and subject to change.\\n\\nExperimenting with AI models in the playground\\nThe AI model playground is a free resource that allows you to adjust model parameters and submit prompts to see how a model responds.\\nNote\\n\\n\\nThe model playground is in public preview and subject to change.\\nThe playground is rate limited. See Rate limits below.\\n\\n\\nTo adjust parameters for the model, in the playground, select the Parameters tab in the sidebar.\\nTo see code that corresponds to the parameters that you selected, switch from the Chat tab to the Code tab.\\n\\nComparing models\\nYou can submit a prompt to two models at the same time and compare the responses.\\nWith one model open in the playground, click Compare, then, in the dropdown menu, select a model for comparison. The selected model opens in a second chat window. When you type a prompt in either chat window, the prompt is mirrored to the other window. The prompts are submitted simultaneously so that you can compare the responses from each model.\\nAny parameters you set are used for both models.\\nEvaluating AI models\\nOnce you\\'ve started testing prompts in the playground, you can evaluate model performance using structured metrics. Evaluations help you compare multiple prompt configurations across different models and determine which setup performs best.\\nIn the Comparisons view, you can apply evaluators like similarity, relevance, and groundedness to measure how well each output meets your expectations. You can also define your own evaluation criteria with a custom prompt evaluator.\\nFor step-by-step instructions, see Evaluating outputs.\\nExperimenting with AI models using the API\\nNote\\n\\nThe free API usage is in public preview and subject to change.\\n\\nGitHub provides free API usage so that you can experiment with AI models in your own application.\\nThe steps to use each model are similar. In general, you will need to:\\n\\n\\nGo to github.com/marketplace/models.\\n\\n\\nClick Model: Select a Model at the top left of the page.\\n\\n\\nChoose a model from the dropdown menu.\\nAlternatively, in the dropdown menu, click View all models, click a model in the Marketplace, then click  Playground.\\nThe model opens in the model playground.\\n\\n\\nClick the Code tab.\\n\\n\\nOptionally, use the language dropdown to select the programming language.\\n\\n\\nOptionally, use the SDK dropdown to select which SDK to use.\\nAll models can be used with the Azure AI Inference SDK, and some models support additional SDKs. If you want to easily switch between models, you should select \"Azure AI Inference SDK.\" If you selected \"REST\" as the language, you won\\'t use an SDK. Instead, you will use the API endpoint directly.  See GitHub Models REST API.\\n\\n\\nEither open a codespace, or set up your local environment:\\n\\nTo run in a codespace, click  Run codespace, then click Create new codespace.\\nTo run locally:\\n\\nCreate a GitHub personal access token. The token needs to have models:read permissions. See Managing your personal access tokens.\\nSave your token as an environment variable.\\nInstall the dependencies for the SDK, if required.\\n\\n\\n\\n\\n\\nUse the example code to make a request to the model.\\n\\n\\nThe free API usage is rate limited. See Rate limits below.\\nSaving and sharing your playground experiments\\nYou can save and share your progress in the playground with presets. Presets save:\\n\\nYour current state\\nYour parameters\\nYour chat history (optional)\\n\\nTo create a preset for your current context, select Preset: PRESET-NAME  at the top right of the playground, then click  Create new preset. You need to name your preset, and you can also choose to provide a preset description, include your chat history, and allow your preset to be shared.\\nThere are two ways to load a preset:\\n\\nSelect the Preset: PRESET-NAME  dropdown menu, then click the preset you want to load.\\nOpen a shared preset URL\\n\\nAfter you load a preset, you can edit, share, or delete the preset:\\n\\nTo edit the preset, change the parameters and prompt the model. Once you are satisfied with your changes, select the Preset: PRESET-NAME  dropdown menu, then click  Edit preset and save your updates.\\nTo share the preset, select the Preset: PRESET-NAME  dropdown menu, then click  Share preset to get a shareable URL.\\nTo delete the preset, select the Preset: PRESET-NAME  dropdown menu, then click  Delete preset and confirm the deletion.\\n\\nUsing the prompt editor\\nThe prompt editor in GitHub Models is designed to help you iterate, refine, and perfect your prompts. This dedicated view provides a focused and intuitive experience for crafting and testing inputs, enabling you to:\\n\\nQuickly test and refine prompts without the complexity of multi-turn interactions.\\nFine-tune prompts for precision and relevance in your projects.\\nUse a specialized space for single-turn scenarios to ensure consistent and optimized results.\\n\\nTo access the prompt editor, click  Prompt editor at the top right of the playground.\\n\\nExperimenting with AI models in Visual Studio Code\\nNote\\n The AI Toolkit extension for Visual Studio Code is in public preview and is subject to change.\\n\\nIf you prefer to experiment with AI models in your IDE, you can install the AI Toolkit extension for Visual Studio Code, then test models with adjustable parameters and context.\\n\\n\\nIn Visual Studio Code, install the pre-release version of the AI Toolkit for Visual Studio Code.\\n\\n\\nTo open the extension, click the AI Toolkit icon in the activity bar.\\n\\n\\nAuthorize the AI Toolkit to connect to your GitHub account.\\n\\n\\nIn the \"My models\" section of the AI Toolkit panel, click Open Model Catalog, then find a model to experiment with.\\n\\nTo use a model hosted remotely through GitHub Models, on the model card, click Try in playground.\\nTo download and use a model locally, on the model card, click Download. Once the download is complete, on the same model card, click Load in playground.\\n\\n\\n\\nIn the sidebar, provide any context instructions and inference parameters for the model, then send a prompt.\\n\\n\\nGoing to production\\nThe free rate limits provided in the playground and API usage are intended to help you get started with experimentation. When you are ready to move beyond the free offering, you have two options for accessing AI models beyond the free limits:\\n\\nYou can opt in to paid usage for GitHub Models, allowing your organization to access increased rate limits, larger context windows, and additional features. See GitHub Models billing.\\nIf you have an existing OpenAI or Azure subscription, you can bring your own API keys (BYOK) to access custom models. Billing and usage are managed directly through your provider account, such as your Azure Subscription ID. See Using your own API keys in GitHub Models.\\n\\nRate limits\\nNote\\n Once you opt in to paid usage, you will have access to production grade rate limits and be billed for all usage thereafter. For more information about these rate limits, see Azure AI Foundry Models quotas and limits in the Azure documentation.\\n\\nThe playground and free API usage are rate limited by requests per minute, requests per day, tokens per request, and concurrent requests. If you get rate limited, you will need to wait for the rate limit that you hit to reset before you can make more requests.\\nLow, high, and embedding models have different rate limits. To see which type of model you are using, refer to the model\\'s information in GitHub Marketplace.\\nFor custom models accessed with your own API keys, rate limits are set and enforced by your model provider.\\n\\n\\nRate limit tier\\nRate limits\\nCopilot Free\\nCopilot Pro\\nCopilot Business\\nCopilot Enterprise\\n\\n\\nLow\\nRequests per minute\\n15\\n15\\n15\\n20\\n\\n\\nRequests per day\\n150\\n150\\n300\\n450\\n\\n\\nTokens per request\\n8000 in, 4000 out\\n8000 in, 4000 out\\n8000 in, 4000 out\\n8000 in, 8000 out\\n\\n\\nConcurrent requests\\n5\\n5\\n5\\n8\\n\\n\\nHigh\\nRequests per minute\\n10\\n10\\n10\\n15\\n\\n\\nRequests per day\\n50\\n50\\n100\\n150\\n\\n\\nTokens per request\\n8000 in, 4000 out\\n8000 in, 4000 out\\n8000 in, 4000 out\\n16000 in, 8000 out\\n\\n\\nConcurrent requests\\n2\\n2\\n2\\n4\\n\\n\\nEmbedding\\nRequests per minute\\n15\\n15\\n15\\n20\\n\\n\\nRequests per day\\n150\\n150\\n300\\n450\\n\\n\\nTokens per request\\n64000\\n64000\\n64000\\n64000\\n\\n\\nConcurrent requests\\n5\\n5\\n5\\n8\\n\\n\\nAzure OpenAI o1-preview\\nRequests per minute\\nNot applicable\\n1\\n2\\n2\\n\\n\\nRequests per day\\nNot applicable\\n8\\n10\\n12\\n\\n\\nTokens per request\\nNot applicable\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 8000 out\\n\\n\\nConcurrent requests\\nNot applicable\\n1\\n1\\n1\\n\\n\\nAzure OpenAI o1, o3, and gpt-5\\nRequests per minute\\nNot applicable\\n1\\n2\\n2\\n\\n\\nRequests per day\\nNot applicable\\n8\\n10\\n12\\n\\n\\nTokens per request\\nNot applicable\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 8000 out\\n\\n\\nConcurrent requests\\nNot applicable\\n1\\n1\\n1\\n\\n\\nAzure OpenAI o1-mini, o3-mini, o4-mini, gpt-5-mini, gpt-5-nano, and gpt-5-chat\\nRequests per minute\\nNot applicable\\n2\\n3\\n3\\n\\n\\nRequests per day\\nNot applicable\\n12\\n15\\n20\\n\\n\\nTokens per request\\nNot applicable\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 4000 out\\n\\n\\nConcurrent requests\\nNot applicable\\n1\\n1\\n1\\n\\n\\nDeepSeek-R1, DeepSeek-R1-0528, and MAI-DS-R1\\nRequests per minute\\n1\\n1\\n2\\n2\\n\\n\\nRequests per day\\n8\\n8\\n10\\n12\\n\\n\\nTokens per request\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 4000 out\\n\\n\\nConcurrent requests\\n1\\n1\\n1\\n1\\n\\n\\nxAI Grok-3\\nRequests per minute\\n1\\n1\\n2\\n2\\n\\n\\nRequests per day\\n15\\n15\\n20\\n30\\n\\n\\nTokens per request\\n4000 in, 4000 out\\n4000 in, 4000 out\\n4000 in, 8000 out\\n4000 in, 16000 out\\n\\n\\nConcurrent requests\\n1\\n1\\n1\\n1\\n\\n\\nxAI Grok-3-Mini\\nRequests per minute\\n2\\n2\\n3\\n3\\n\\n\\nRequests per day\\n30\\n30\\n40\\n50\\n\\n\\nTokens per request\\n4000 in, 8000 out\\n4000 in, 8000 out\\n4000 in, 12000 out\\n4000 in, 12000 out\\n\\n\\nConcurrent requests\\n1\\n1\\n1\\n1\\n\\n\\nThese limits are subject to change without notice.\\nLeaving feedback\\nTo ask questions and share feedback, see this GitHub Models discussion post.\\nTo learn how others are using GitHub Models, visit the GitHub Community discussions for Models.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()  # This will load the content from the specified web page.\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
